\section{Transformers}
Transformer were first introduced in 2017 as successor to neural machine translation  (NMT) algorithms such as  long short-term memory (LSTM) and Recurrent Neural Networks (RNNs) \cite{Vaswani2017}. 

Transformers solve several problems that previous state-of-the-art algorithms such as long short-term memory (LSTM) and Recurrent Neural Networks (RNNs) have struggled with, such as computational efficiency and context window size \cite[p~609]{Geron2022}.

These have the problem, that they can only look back a limited amount of words, and everything not being inside the current window was not utilized to generate the next word.
Transformers however make use of so called self-attention

Attention in the context of machine learning (ML) is a way to determine which 
\section{Vision Transformer}
\section{Dino and DinoV2}
\subsection{Depth Estimation}
\subsection{Segmentation}

\subsection{Low Rank Adaption (LoRA)}
